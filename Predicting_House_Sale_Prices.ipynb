{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting House Sale Prices\n",
    "\n",
    "## Introducing the housing data for the city of Ames, Iowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"AmesHousing.tsv\", delimiter= \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57088.251612639091"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_features(df):\n",
    "    return df\n",
    "\n",
    "def select_features(df):\n",
    "    return df[['Gr Liv Area', 'SalePrice']]\n",
    "\n",
    "def train_and_test(df):\n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    features = numeric_train.columns.drop(\"SalePrice\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train[features], train[\"SalePrice\"])\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse   \n",
    "\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "* Handle missing values:\n",
    "  * All columns:\n",
    "        Drop any with 5% or more missing values for now.\n",
    "  * Text columns:\n",
    "        Drop any with 1 or more missing values for now.\n",
    "  * Numerical columns:\n",
    "        For columns with missing values, fill in with the most common value in that column\n",
    "\n",
    "\n",
    "1: All columns: Drop any with 5% or more missing values for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Series object: column name -> number of missing values\n",
    "num_missing_all = df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter Series to columns containing >5% missing values\n",
    "cols_to_drop = num_missing_all[num_missing_all>(len(df)/20)].index\n",
    "\n",
    "# Drop those columns from the data frame.\n",
    "df = df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Text columns: Drop any with 1 or more missing values for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Series object: column name -> number of missing values\n",
    "text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "missing_text_count= df[text_cols].isnull().sum()\n",
    "\n",
    "# Filter Series to columns containing *any* missing values\n",
    "text_cols_to_drop= missing_text_count[missing_text_count>0].index\n",
    "df= df.drop(text_cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Numerical columns: For columns with missing values, fill in with the most common value in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BsmtFin SF 1       1\n",
       "BsmtFin SF 2       1\n",
       "Bsmt Unf SF        1\n",
       "Total Bsmt SF      1\n",
       "Garage Cars        1\n",
       "Garage Area        1\n",
       "Bsmt Full Bath     2\n",
       "Bsmt Half Bath     2\n",
       "Mas Vnr Area      23\n",
       "dtype: int64"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute column-wise missing value counts\n",
    "num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "fixable_numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bsmt Full Bath': 0.0,\n",
       " 'Bsmt Half Bath': 0.0,\n",
       " 'Bsmt Unf SF': 0.0,\n",
       " 'BsmtFin SF 1': 0.0,\n",
       " 'BsmtFin SF 2': 0.0,\n",
       " 'Garage Area': 0.0,\n",
       " 'Garage Cars': 2.0,\n",
       " 'Mas Vnr Area': 0.0,\n",
       " 'Total Bsmt SF': 0.0}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the most common value for each column in `fixable_nmeric_missing_cols`.\n",
    "replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "replacement_values_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values with the most common values for each column.\n",
    "df = df.fillna(replacement_values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    64\n",
       "dtype: int64"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that every column has 0 missing values\n",
    "df.isnull().sum().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new features that better capture the information in some of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2180   -1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years_sold = df['Yr Sold'] - df['Year Built']\n",
    "years_sold[years_sold < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1702   -1\n",
       "2180   -2\n",
       "2181   -1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years_since_remod = df['Yr Sold'] - df['Year Remod/Add']\n",
    "years_since_remod[years_since_remod < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new columns for \"Years Before Sale\" and \"Years since Remod\"\n",
    "df['Years Before Sale'] = years_sold\n",
    "df['Years Since Remod'] = years_since_remod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with negative values for both these features. \n",
    "df= df.drop([1702,2180,2181], axis=0)\n",
    "\n",
    "# No longer need original year columns\n",
    "df = df.drop([\"Year Built\", \"Year Remod/Add\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns that:\n",
    "\n",
    "    * that aren't useful for ML\n",
    "    * leak data about the final sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop columns that aren't useful for ML\n",
    "df = df.drop([\"PID\", \"Order\"], axis=1)\n",
    "\n",
    "# Drop columns that leak info about the final sale\n",
    "df = df.drop([\"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Yr Sold\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating transform_features() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55275.367312413073"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_features(df):\n",
    "    num_missing_all = df.isnull().sum()\n",
    "    cols_to_drop = num_missing_all[num_missing_all>(len(df)/20)].index\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    missing_text_count= df[text_cols].isnull().sum()\n",
    "    text_cols_to_drop= missing_text_count[missing_text_count>0].index\n",
    "    df= df.drop(text_cols_to_drop, axis=1)\n",
    "    \n",
    "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "        \n",
    "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "    df = df.fillna(replacement_values_dict)\n",
    "    \n",
    "    df['Years Before Sale'] = df['Yr Sold'] - df['Year Built']\n",
    "    df['Years Since Remod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    \n",
    "    df= df.drop([1702,2180,2181], axis=0)\n",
    "    df = df.drop([\"PID\", \"Order\", \"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Year Built\", \"Year Remod/Add\"], axis = 1)\n",
    "    return df\n",
    "\n",
    "def select_features(df):\n",
    "    return df[['Gr Liv Area', 'SalePrice']]\n",
    "\n",
    "def train_and_test(df):\n",
    "    train = df[:1460]\n",
    "    test = df[1460:]\n",
    "    numeric_train = train.select_dtypes(include=['integer', 'float'])\n",
    "    numeric_test = test.select_dtypes(include=['integer', 'float'])\n",
    "    \n",
    "    features = numeric_train.columns.drop(\"SalePrice\")\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(train[features], train[\"SalePrice\"])\n",
    "    predictions = lr.predict(test[features])\n",
    "    mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse   \n",
    "\n",
    "df = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PID               -0.246521\n",
       "Enclosed Porch    -0.128787\n",
       "Kitchen AbvGr     -0.119814\n",
       "Overall Cond      -0.101697\n",
       "MS SubClass       -0.085092\n",
       "Low Qual Fin SF   -0.037660\n",
       "Bsmt Half Bath    -0.035835\n",
       "Order             -0.031408\n",
       "Yr Sold           -0.030569\n",
       "Misc Val          -0.015691\n",
       "BsmtFin SF 2       0.005891\n",
       "3Ssn Porch         0.032225\n",
       "Mo Sold            0.035259\n",
       "Pool Area          0.068403\n",
       "Screen Porch       0.112151\n",
       "Bedroom AbvGr      0.143913\n",
       "Bsmt Unf SF        0.182855\n",
       "Lot Area           0.266549\n",
       "2nd Flr SF         0.269373\n",
       "Bsmt Full Bath     0.276050\n",
       "Half Bath          0.285056\n",
       "Open Porch SF      0.312951\n",
       "Wood Deck SF       0.327143\n",
       "Lot Frontage       0.357318\n",
       "BsmtFin SF 1       0.432914\n",
       "Fireplaces         0.474558\n",
       "TotRms AbvGrd      0.495474\n",
       "Mas Vnr Area       0.508285\n",
       "Garage Yr Blt      0.526965\n",
       "Year Remod/Add     0.532974\n",
       "Full Bath          0.545604\n",
       "Year Built         0.558426\n",
       "1st Flr SF         0.621676\n",
       "Total Bsmt SF      0.632280\n",
       "Garage Area        0.640401\n",
       "Garage Cars        0.647877\n",
       "Gr Liv Area        0.706780\n",
       "Overall Qual       0.799262\n",
       "SalePrice          1.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_df = df.select_dtypes(include=['integer', 'float'])\n",
    "corr_mat= numerical_df.corr()[\"SalePrice\"].sort_values()\n",
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BsmtFin SF 2       0.005891\n",
       "Misc Val           0.015691\n",
       "Yr Sold            0.030569\n",
       "Order              0.031408\n",
       "3Ssn Porch         0.032225\n",
       "Mo Sold            0.035259\n",
       "Bsmt Half Bath     0.035835\n",
       "Low Qual Fin SF    0.037660\n",
       "Pool Area          0.068403\n",
       "MS SubClass        0.085092\n",
       "Overall Cond       0.101697\n",
       "Screen Porch       0.112151\n",
       "Kitchen AbvGr      0.119814\n",
       "Enclosed Porch     0.128787\n",
       "Bedroom AbvGr      0.143913\n",
       "Bsmt Unf SF        0.182855\n",
       "PID                0.246521\n",
       "Lot Area           0.266549\n",
       "2nd Flr SF         0.269373\n",
       "Bsmt Full Bath     0.276050\n",
       "Half Bath          0.285056\n",
       "Open Porch SF      0.312951\n",
       "Wood Deck SF       0.327143\n",
       "Lot Frontage       0.357318\n",
       "BsmtFin SF 1       0.432914\n",
       "Fireplaces         0.474558\n",
       "TotRms AbvGrd      0.495474\n",
       "Mas Vnr Area       0.508285\n",
       "Garage Yr Blt      0.526965\n",
       "Year Remod/Add     0.532974\n",
       "Full Bath          0.545604\n",
       "Year Built         0.558426\n",
       "1st Flr SF         0.621676\n",
       "Total Bsmt SF      0.632280\n",
       "Garage Area        0.640401\n",
       "Garage Cars        0.647877\n",
       "Gr Liv Area        0.706780\n",
       "Overall Qual       0.799262\n",
       "SalePrice          1.000000\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_corr_coeffs= numerical_df.corr()[\"SalePrice\"].abs().sort_values()\n",
    "abs_corr_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BsmtFin SF 2       0.005891\n",
       "Misc Val           0.015691\n",
       "Yr Sold            0.030569\n",
       "Order              0.031408\n",
       "3Ssn Porch         0.032225\n",
       "Mo Sold            0.035259\n",
       "Bsmt Half Bath     0.035835\n",
       "Low Qual Fin SF    0.037660\n",
       "Pool Area          0.068403\n",
       "MS SubClass        0.085092\n",
       "Overall Cond       0.101697\n",
       "Screen Porch       0.112151\n",
       "Kitchen AbvGr      0.119814\n",
       "Enclosed Porch     0.128787\n",
       "Bedroom AbvGr      0.143913\n",
       "Bsmt Unf SF        0.182855\n",
       "PID                0.246521\n",
       "Lot Area           0.266549\n",
       "2nd Flr SF         0.269373\n",
       "Bsmt Full Bath     0.276050\n",
       "Half Bath          0.285056\n",
       "Open Porch SF      0.312951\n",
       "Wood Deck SF       0.327143\n",
       "Lot Frontage       0.357318\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_corr_coeffs[abs_corr_coeffs<0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop columns with less than 0.4 correlation with SalePrice.\n",
    "columns= abs_corr_coeffs[abs_corr_coeffs < 0.4].index\n",
    "for col in columns:\n",
    "    if col in transform_df.columns:\n",
    "        transform_df= transform_df.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which categorical columns should we keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of column names that are categorical.\n",
    "nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don't have any semantic meaning)?\n",
    "* If a categorical column has hundreds of unique values (or categories), should we keep it? When we dummy code this column, hundreds of columns will need to be added back to the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MS Zoning',\n",
       " 'Street',\n",
       " 'Land Contour',\n",
       " 'Lot Config',\n",
       " 'Neighborhood',\n",
       " 'Condition 1',\n",
       " 'Condition 2',\n",
       " 'Bldg Type',\n",
       " 'House Style',\n",
       " 'Roof Style',\n",
       " 'Roof Matl',\n",
       " 'Exterior 1st',\n",
       " 'Exterior 2nd',\n",
       " 'Foundation',\n",
       " 'Heating',\n",
       " 'Central Air']"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which categorical columns have we still carried with us? \n",
    "transform_cat_cols = []\n",
    "for col in nominal_features:\n",
    "    if col in transform_df.columns:\n",
    "        transform_cat_cols.append(col)\n",
    "\n",
    "transform_cat_cols        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Street           2\n",
       "Central Air      2\n",
       "Land Contour     4\n",
       "Lot Config       5\n",
       "Bldg Type        5\n",
       "Roof Style       6\n",
       "Foundation       6\n",
       "Heating          6\n",
       "MS Zoning        7\n",
       "Condition 2      8\n",
       "House Style      8\n",
       "Roof Matl        8\n",
       "Condition 1      9\n",
       "Exterior 1st    16\n",
       "Exterior 2nd    17\n",
       "Neighborhood    28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many unique values in each categorical column?\n",
    "uniqueness_counts = transform_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "uniqueness_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aribtrary cutoff of 10 unique values.\n",
    "drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
    "transform_df = transform_df.drop(drop_nonuniq_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select just the remaining text columns and convert to categorical.\n",
    "text_cols = transform_df.select_dtypes(include=['object']).columns\n",
    "for col in text_cols:\n",
    "    transform_df[col] = transform_df[col].astype('category')\n",
    "    \n",
    "# Create dummy columns and add back to the dataframe!\n",
    "transform_df = pd.concat([\n",
    "    transform_df, \n",
    "    pd.get_dummies(transform_df.select_dtypes(include=['category']))\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating select_features function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_features(df):\n",
    "    num_missing_all = df.isnull().sum()\n",
    "    cols_to_drop = num_missing_all[num_missing_all>(len(df)/20)].index\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    missing_text_count= df[text_cols].isnull().sum()\n",
    "    text_cols_to_drop= missing_text_count[missing_text_count>0].index\n",
    "    df= df.drop(text_cols_to_drop, axis=1)\n",
    "    \n",
    "    num_missing = df.select_dtypes(include=['int', 'float']).isnull().sum()\n",
    "    fixable_numeric_cols = num_missing[(num_missing < len(df)/20) & (num_missing > 0)].sort_values()\n",
    "        \n",
    "    replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient='records')[0]\n",
    "    df = df.fillna(replacement_values_dict)\n",
    "    \n",
    "    df['Years Before Sale'] = df['Yr Sold'] - df['Year Built']\n",
    "    df['Years Since Remod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
    "    \n",
    "    df= df.drop([1702,2180,2181], axis=0)\n",
    "    df = df.drop([\"PID\", \"Order\", \"Mo Sold\", \"Sale Condition\", \"Sale Type\", \"Year Built\", \"Year Remod/Add\"], axis = 1)\n",
    "    return df\n",
    "\n",
    "def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n",
    "    numerical_df = df.select_dtypes(include=['int', 'float'])\n",
    "    abs_corr_coeffs = numerical_df.corr()['SalePrice'].abs().sort_values()\n",
    "    df = df.drop(abs_corr_coeffs[abs_corr_coeffs < coeff_threshold].index, axis=1)\n",
    "    \n",
    "    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
    "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
    "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
    "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
    "    \n",
    "    transform_cat_cols = []\n",
    "    for col in nominal_features:\n",
    "        if col in df.columns:\n",
    "            transform_cat_cols.append(col)\n",
    "\n",
    "    uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values()\n",
    "    drop_nonuniq_cols = uniqueness_counts[uniqueness_counts > 10].index\n",
    "    df = df.drop(drop_nonuniq_cols, axis=1)\n",
    "    \n",
    "    text_cols = df.select_dtypes(include=['object'])\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].astype('category')\n",
    "    df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=['category']))], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_and_test(df, k=0):\n",
    "    numeric_df = df.select_dtypes(include=['integer', 'float'])\n",
    "    features = numeric_df.columns.drop(\"SalePrice\")\n",
    "    lr = LinearRegression()\n",
    "    \n",
    "    # When k equals 0, holdout validation .\n",
    "    if k==0:\n",
    "        train = df[:1460]\n",
    "        test = df[1460:]  \n",
    "        lr.fit(train[features], train[\"SalePrice\"])\n",
    "        predictions = lr.predict(test[features])\n",
    "        mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse   \n",
    "    \n",
    "    # When k equals 1, perform simple cross validation\n",
    "    if k==1: \n",
    "        shuffled_df = df.iloc[np.random.permutation(len(numeric_df))]\n",
    "        fold_one = shuffled_df[0:1460]\n",
    "        fold_two= shuffled_df[1460:]\n",
    "        rmses=[]\n",
    "        \n",
    "        lr.fit(fold_one[features], fold_one[\"SalePrice\"])\n",
    "        predictions = lr.predict(fold_two[features])\n",
    "        mse_one = mean_squared_error(fold_two[\"SalePrice\"], predictions)\n",
    "        rmse_one = np.sqrt(mse_one)\n",
    "        rmses.append(rmse_one)\n",
    "        \n",
    "        lr.fit(fold_two[features], fold_two[\"SalePrice\"])\n",
    "        predictions = lr.predict(fold_one[features])\n",
    "        mse_two = mean_squared_error(fold_one[\"SalePrice\"], predictions)\n",
    "        rmse_two = np.sqrt(mse_one)\n",
    "        rmses.append(rmse_two)\n",
    "        \n",
    "        avg_rmse = np.mean(rmses)\n",
    "        return avg_rmse\n",
    "    \n",
    "    # When K>1 implement k-fold cross validation using k folds\n",
    "    else: \n",
    "        kf= KFold(n_splits=k, shuffle=True)\n",
    "        rmse_values=[]\n",
    "        for train_index, test_index in kf.split(df):\n",
    "            train= df.iloc[train_index]\n",
    "            test = df.iloc[test_index]\n",
    "            lr.fit(train[features], train[\"SalePrice\"])\n",
    "            predictions = lr.predict(test[features])\n",
    "            mse = mean_squared_error(test[\"SalePrice\"], predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            rmse_values.append(rmse)\n",
    "        print(rmse_values)\n",
    "        avg_rmse= np.mean(rmse_values)\n",
    "        return avg_rmse       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27824.903234131503, 36612.017868973628, 25767.771045441234, 26462.813070505119]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29166.876304762871"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df, k=4)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33367.287183402834"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df, k=0)\n",
    "\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26075.079681164883"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"AmesHousing.tsv\", delimiter=\"\\t\")\n",
    "transform_df = transform_features(df)\n",
    "filtered_df = select_features(transform_df)\n",
    "rmse = train_and_test(filtered_df, k=1)\n",
    "\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
